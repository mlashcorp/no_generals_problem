[{"content":" Recently, I got very interested in learning and understanding how LLMs work. Specifically, I was curious about how LLMs are used at scale (i.e., with multiple nodes).\nOne of the best resources I found to learn about LLMs was Andrej Karpathy\u0026rsquo;s YouTube video, where he created GPT-2 from scratch .\nAlso, if, like me, your fundamentals on neural networks are a bit rusty, he has a whole series where he builds up from the basic concepts of neural nets up to the transformer model.\n—\nHaving learned the basics of how a Transformer model works, it was time to dive into the gory details of distributing the computation of an LLM.\nMy intuition told me to start with the problem of distributing the forward pass (inference, in which we provide input to an LLM and get generated text as a result), if nothing else, because it felt like an easier problem to tackle compared to distributed training.\nMost of the work I\u0026rsquo;ve seen focuses on Training parallelism - using multiple machines to accelerate the process of training the transformer model. However, training a neural network requires both forward and backward passes, so we should be able to leverage existing techniques used for training to our goal of distributing inference.\nTo me, it makes intuitive sense that most information about LLM parallelism is related to training. This is a non-interactive operation (not sensitive to latency) that is quite compute-intensive. LLM Inference, on the other hand, is typically used in applications where the time to response is critical (such as chatGPT) - in a distributed setting, unless the problem we are tackling is embarrassingly parallelizable, we can expect the need to synchronize data between nodes, which will add latency to our application. However, I believe there are still several applications that would benefit from distributing inference at the cost of latency (such as batch operations that require LLM capabilities), and I\u0026rsquo;m also keen to explore how fast I can perform inference using multiple nodes.\nIn this blog series, I will explore existing algorithms to perform distributed inference for LLMs, their limitations and tradeoffs, and (try to) implement them from scratch to understand them better.\nHow many ways can you split a matrix? Naturally, there already exists plenty of work dedicated to distributing the training of LLMs, and as I said, many of the concepts and techniques developed for parallel training should be applicable to inference as well. OpenAI provides a great starting point to understand how researchers in the field have tackled this problem.\nThere are several techniques to perform parallel training of LLMs, but for the sake of simplicity, I will start by dividing the problem into 3 options:\nData Parallelism - where we load the entire model in each node but only pass part of the training data. Gradients are then averaged across workers. This option is not applicable for inference since LLMs are auto-regressive and, as such, require previously generated tokens to predict a given token.\nPipeline Parallelism - here, we load a subset of the model\u0026rsquo;s layers in each node and sequentially pass activations until we reach the network\u0026rsquo;s end. Intuitively, because there is a sequential dependency between nodes, we can expect that there will be no gains from concurrently processing data in multiple machines. However, we can split a large model that would typically not fit in memory in multiple machines since the parameter size per node should be 1/n for n nodes.\nTensor Parallelism - in tensor parallelism, we leverage the fact that matrix multiplication is a problem that is trivially parallelizable. Each node maintains all layers of the network, but for each layer, only a fraction of the parameters. In this configuration, we can process each layer in parallel in multiple nodes, reducing the memory usage and the compute requirements in each node.\nThere are other ways to distribute the computation of LLMs, but for now, I will focus first on understanding tensor parallelism, specifically the MegatronLM paper from Nvidia.\nTensor Parallelism with MegatronLM One of the key ideas of the MegatronLM paper is that we can leverage the mathematical properties of matrix multiplication to distribute our computation. The paper focuses on applying their ideas to the transformer model. Specifically, we will be applying these ideas to the GPT-2 architecture, which is a decoder-only transformer.\nTo better understand the paper, I implemented some ideas using Andrej\u0026rsquo;s GPT-2 nanoGPT implementation as a reference. The code can be found here, and I will reference it as I go through the paper.\n—\nDecoder transformers are composed of several blocks of self-attention and fully connected layers. We will focus on section 3 of the paper, where the authors provide an implementation both for the fully connected layer (MLP) and for the attention layer.\nParallelizing the MLP A multilayer perceptron is a kind of neural network. It is a fully connected network with a non-linear activation function. In GPT-2, this MLP has 2 linear layers with a GeLU non-linearity in between them.\n1 2 3 4 5 def forward(self, x): _, T, _ = x.size() x = self.c_fc(x) # linear layer x = self.gelu(x) # activation x = self.c_proj(x) # linear layer The key idea from the paper is that we can split the parameters of each linear layer in this MLP across nodes, but how we split the parameters and reconcile the results is worth explaining.\nThe following illustration shows a simple representation of a multiplication of an input 1x2 matrix by a 2x2 matrix, the result of which will be a 1x2 matrix.\nIf we split the 2x2 matrix across its rows, each node would have a 1x2 result matrix, but the values would not be correct until we summed the matrices from both nodes. If, on the other hand, we split the 2x2 matrix across its columns, as shown in the illustration above, each node will have a 1x1 matrix with the correct final result.\nIn the transformer MLP, we can think of the blue matrix as the input and the 2x2 matrix (in yellow and red) as the parameters of the linear layer. The number of columns represents the hidden dimension of the linear layer, and the number of rows must match the columns of our input. The GPT-2 paper actually uses 4 * n_embed for the hidden dimension, but for simplicity\u0026rsquo;s sake, I\u0026rsquo;ll use n_hidden = n_embed.\nTo calculate the output of the MLP, we need to multiply the two matrices. This is where we can split the 2x2 matrix across the two columns sending each split to a different node, then send the full input matrix to both nodes, and finally, in each node, independently (and in parallel), calculate each result element (green and pink elements in the figure).\nThis approach splits the parameter matrix of the linear layer across multiple nodes, reducing the amount of memory and computation that each node must do. The entire input matrix must still be passed to all nodes, so we get sublinear distribution gains from this operation.\nThe diagram above represents part of Figure 3a from the paper. In it, the authors explain the key idea of first splitting the A parameter matrix by columns and the B matrix by rows. As we saw before, if we split the second element of matrix multiplication by columns, each node will have a result matrix with a subset of the columns, whereas if we split by rows, the result matrix will have the correct result shape, but we will need to reduce (synchronize) the data between all nodes to arrive at the correct result.\nThe authors opted for this order of operations to minimize synchronization points. A GeLU is a non-linear operation that can only be safely applied in parallel if we split the first linear layer across its columns, resulting in the equation:\n$$ [Y1, Y2] = [GeLU(X A1), GeLU(X A2)] $$\nWhere A is the parameters of the linear layer, A1 has the first half of the columns of the matrix, and A2 is the second half. As we saw before, we can independently calculate the resulting Y matrix in different nodes.\nHad the authors split A across its rows, they would need to synchronize data before applying the non-linear activation function - since each node would have a result matrix with the correct format after the first linear layer, but with only half of the parameters having been summed.\nFor the second linear layer, we must partition its parameter matrix along its rows (matrices B1 and B2) so that matrix multiplication rules are preserved.\nAfter applying the non-linearity, each node will have a resulting 1x2 matrix (lime green and brown in node 1, dark red and magenta in node 2) that contains the result in the correct format, but to calculate the final result of the MLP block, we must take both matrices from the 2 nodes and add them (using an all_reduce operation) to get the final result.\nThis is the key strategy that the authors also use in the self-attention block. I won\u0026rsquo;t go into details of that here, but will briefly mention it as we go through the code.\nAdapting NanoGPT to run in parallel To apply the ideas discussed in the previous section, I adapted the NanoGPT implementation so that the MLP and the self-attention blocks can be split across several nodes. My goal was to create a minimally viable implementation, that favors readibility against completeness or sophistication.\nThe entire GPT-2 implementation is still contained in a single file, distributed_model.py. The relevant files from the repo are:\n1 2 3 4 5 6 . ├── run.py \u0026lt;- CLI application wrapper ├── distributed_inference.py \u0026lt;- Launches the processes and waits for the results. ├── distributed_model.py \u0026lt;- GPT-2 with tensor parallelism ├── distributed_state.py \u0026lt;- Simulated distributed all reduce └── download_model.py \u0026lt;- Use this to download the GPT-2 124M model from HF As an example, for this first PoC, I\u0026rsquo;m not using Torch\u0026rsquo;s all_reduce operation, but rather simulating it using Python\u0026rsquo;s BaseManager process. The idea here is that we launch several processes, and they all publish their intermediate results to the base manager, and only when all have submited their results, they all get the final, reduced version of the matrix:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 def all_reduce(self, op_id: str, tensor: torch.Tensor): with self.cv: if op_id not in self.state: self.state[op_id] = (tensor, 1) else: state_tensor, count = self.state[op_id] self.state[op_id] = ( torch.add(state_tensor, tensor), count + 1) if self.state[op_id][1] \u0026lt; self.number_of_workers: self.cv.wait() else: self.cv.notify_all() return self.state[op_id][0] For each operation (uniquely identified using the op_id - using the worker index, the block name, and the current token index) we sum the input matrix with the existing state, and block the caller on a condition variable. When the final worker calls the all_reduce function, we notify all blocked users and return the final matrix.\nThe points where we use all reduce are the synchronization points between machines. The amount of data we need to transfer and the inter-node latency will be the bottlenecks for the forward pass performance.\nThis first version of the code does not address all aspects presented in the paper. I simply focused on implementing the distributed MLP and self-attention blocks. Other aspects such as word, position and transposed embeddings were not distributed yet.\nThere are 2 key areas worth exploring in this code. 1) how I\u0026rsquo;m loading the model shard in each node; and 2) how I\u0026rsquo;m setting the MLP and Self-Attention parameter sizes. Let\u0026rsquo;s look at model loading first.\nSharded model loading Model loading is done in the load_layers function of the distributed_mode.py module. I leverage safetensors to load a slice of each layer selectively. As an example:\n1 2 3 4 if \u0026#34;mlp.c_fc.weight\u0026#34; in layer: # Partition by column. This will convert the slice to a Tensor object tensor_slice = f.get_slice(layer) tensors[layer] = tensor_slice[:, mlp_start_idx:mlp_end_idx] Here I\u0026rsquo;m loading a slice of the first linear layer of the MLP block by loading only the columns allocated to this worker node. Start and end indeces for the columns are calculated using this helper function:\n1 2 3 4 5 6 7 8 def get_worker_partition(C: int = 768, worker_index: int = 0, number_of_workers: int = 1): partition_size = C // number_of_workers # Calculate the \u0026#34;chunk\u0026#34; that this node will process partition_start = worker_index * partition_size partition_end = partition_start + partition_size return (partition_start, partition_end) ","permalink":"https://mlashcorp.github.io/no_generals_problem/posts/1/","summary":"Recently, I got very interested in learning and understanding how LLMs work. Specifically, I was curious about how LLMs are used at scale (i.e., with multiple nodes).\nOne of the best resources I found to learn about LLMs was Andrej Karpathy\u0026rsquo;s YouTube video, where he created GPT-2 from scratch .\nAlso, if, like me, your fundamentals on neural networks are a bit rusty, he has a whole series where he builds up from the basic concepts of neural nets up to the transformer model.","title":"LLM Distributed Inference for Fun and not Profit - Part 1"}]