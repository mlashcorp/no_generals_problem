[{"content":" Recently, I got very interested in learning and understanding how LLMs work. Specifically, I was curious about how LLMs are used at scale (i.e., with multiple nodes).\nOne of the best resources I found to learn about LLMs was Andrej Karpathy\u0026rsquo;s YouTube video, where he creates GPT-2 from scratch .\nAlso, if, like me, your fundamentals on neural networks are a bit rusty, he has a whole series where he builds up from the basic concepts of neural nets up to the transformer model.\nâ€”\nHaving learned the basics of how a Transformer model works, it was time to dive into the gory details of distributing the computation of an LLM.\nMy intuition told me to start with the problem of distributing the forward pass (inference, in which we provide input to an LLM and get generated text as a result) if nothing else because it felt like an easier problem to tackle compared to distributed training.\nMost of the work I\u0026rsquo;ve seen focuses on Training parallelism - using multiple machines to accelerate the process of training the transformer model. However, training a neural network requires both forward and backward passes, so we should be able to leverage existing techniques used for training to our goal of distributing inference.\nTo me, it makes intuitive sense that most information about LLM parallelism is related to training. This is a non-interactive operation (not sensitive to latency) that is quite compute intensive. LLM Inference on the other hand is typically used in applications where the time to response is critical (such as chatGPT) - in a distributed setting, unless the problem we are tackling is embarrassingly parallelizable, we can expect the need to synchronize data between nodes, which will add latency to our application.\nIn this blog series, I will explore existing algorithms to perform distributed inference for LLMs, their limitations and tradeoffs, and (try to) implement them from scratch to better understand them.\nHow many ways can you split a matrix? Naturally, there already exists plenty of work dedicated to distribute the training of LLMs, and as I said, many of the concepts and techniques developed for parallel training should be applicable to inference as well. OpenAI provide a great starting point to understand some of the ways researchers in the field tackled this problem.\nTo simplify the topic, I will start by dividing the problem into 3 options:\nData Parallelism - where we load the entire model in each node, but only pass part of the training data. Gradients are then averaged across workers. This option is not applicable for inference since LLMs are auto-regressive.\nPipeline Parallelism - here we load a subset of the model\u0026rsquo;s layers in each node, and sequentially pass activations until we reach the end of the network. Intuitively, because there is a sequential dependency between nodes, we can expect that there will be no gains from concurrently processing data in multiple machines. However, we will be able to split a large model that would typically not fit in memory in multiple machines, since the parameter size per node should be 1/n for n nodes.\nTensor Parallelism - in tensor parallelism we leverage the fact that matrix multiplication is problem that is trivially parallelizable. Each node maintains all layers of the network, but for each layer only a fraction of the parameters.In this configuration, we can process each layer in parallel in multiple nodes, both reducing the memory usage and the compute requirements in each node.\nThere are other ways to distribute the computation of LLMs, but for now I will focus first on understanding tensor parallelism, specifically, the MegatronLM paper from Nvidia.\n","permalink":"https://mlashcorp.github.io/no_generals_problem/posts/post-1/","summary":"Recently, I got very interested in learning and understanding how LLMs work. Specifically, I was curious about how LLMs are used at scale (i.e., with multiple nodes).\nOne of the best resources I found to learn about LLMs was Andrej Karpathy\u0026rsquo;s YouTube video, where he creates GPT-2 from scratch .\nAlso, if, like me, your fundamentals on neural networks are a bit rusty, he has a whole series where he builds up from the basic concepts of neural nets up to the transformer model.","title":"LLM Distributed Inference for Fun and not Profit - Part 1"}]