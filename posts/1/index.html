<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>LLM Distributed Inference for Fun and not Profit - Part 1 | NGP</title><meta name=keywords content><meta name=description content="Recently, I got very interested in learning and understanding how LLMs work. Specifically, I was curious about how LLMs are used at scale (i.e., with multiple nodes).
One of the best resources I found to learn about LLMs was Andrej Karpathy&rsquo;s YouTube video, where he created GPT-2 from scratch .
Also, if, like me, your fundamentals on neural networks are a bit rusty, he has a whole series where he builds up from the basic concepts of neural nets up to the transformer model."><meta name=author content="José Côrte-Real"><link rel=canonical href=https://mlashcorp.github.io/no_generals_problem/posts/1/><link crossorigin=anonymous href=/no_generals_problem/assets/css/stylesheet.da40ba41337d59f3029c25d46c0720b2220e3cafed447b496406d6d9c94c485f.css integrity="sha256-2kC6QTN9WfMCnCXUbAcgsiIOPK/tRHtJZAbW2clMSF8=" rel="preload stylesheet" as=style><link rel=icon href=https://mlashcorp.github.io/no_generals_problem/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://mlashcorp.github.io/no_generals_problem/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://mlashcorp.github.io/no_generals_problem/favicon-32x32.png><link rel=apple-touch-icon href=https://mlashcorp.github.io/no_generals_problem/apple-touch-icon.png><link rel=mask-icon href=https://mlashcorp.github.io/no_generals_problem/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-L8PPH1DLSB"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-L8PPH1DLSB",{anonymize_ip:!1})}</script><meta property="og:title" content="LLM Distributed Inference for Fun and not Profit - Part 1"><meta property="og:description" content="Recently, I got very interested in learning and understanding how LLMs work. Specifically, I was curious about how LLMs are used at scale (i.e., with multiple nodes).
One of the best resources I found to learn about LLMs was Andrej Karpathy&rsquo;s YouTube video, where he created GPT-2 from scratch .
Also, if, like me, your fundamentals on neural networks are a bit rusty, he has a whole series where he builds up from the basic concepts of neural nets up to the transformer model."><meta property="og:type" content="article"><meta property="og:url" content="https://mlashcorp.github.io/no_generals_problem/posts/1/"><meta property="og:image" content="https://mlashcorp.github.io/no_generals_problem/papermod-cover.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-10-21T16:31:22+01:00"><meta property="article:modified_time" content="2023-10-21T16:31:22+01:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://mlashcorp.github.io/no_generals_problem/papermod-cover.png"><meta name=twitter:title content="LLM Distributed Inference for Fun and not Profit - Part 1"><meta name=twitter:description content="Recently, I got very interested in learning and understanding how LLMs work. Specifically, I was curious about how LLMs are used at scale (i.e., with multiple nodes).
One of the best resources I found to learn about LLMs was Andrej Karpathy&rsquo;s YouTube video, where he created GPT-2 from scratch .
Also, if, like me, your fundamentals on neural networks are a bit rusty, he has a whole series where he builds up from the basic concepts of neural nets up to the transformer model."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://mlashcorp.github.io/no_generals_problem/posts/"},{"@type":"ListItem","position":3,"name":"LLM Distributed Inference for Fun and not Profit - Part 1","item":"https://mlashcorp.github.io/no_generals_problem/posts/1/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"LLM Distributed Inference for Fun and not Profit - Part 1","name":"LLM Distributed Inference for Fun and not Profit - Part 1","description":"Recently, I got very interested in learning and understanding how LLMs work. Specifically, I was curious about how LLMs are used at scale (i.e., with multiple nodes).\nOne of the best resources I found to learn about LLMs was Andrej Karpathy\u0026rsquo;s YouTube video, where he created GPT-2 from scratch .\nAlso, if, like me, your fundamentals on neural networks are a bit rusty, he has a whole series where he builds up from the basic concepts of neural nets up to the transformer model.","keywords":[],"articleBody":" Recently, I got very interested in learning and understanding how LLMs work. Specifically, I was curious about how LLMs are used at scale (i.e., with multiple nodes).\nOne of the best resources I found to learn about LLMs was Andrej Karpathy’s YouTube video, where he created GPT-2 from scratch .\nAlso, if, like me, your fundamentals on neural networks are a bit rusty, he has a whole series where he builds up from the basic concepts of neural nets up to the transformer model.\n—\nHaving learned the basics of how a Transformer model works, it was time to dive into the gory details of distributing the computation of an LLM.\nMy intuition told me to start with the problem of distributing the forward pass (inference, in which we provide input to an LLM and get generated text as a result), if nothing else, because it felt like an easier problem to tackle compared to distributed training.\nMost of the work I’ve seen focuses on Training parallelism - using multiple machines to accelerate the process of training the transformer model. However, training a neural network requires both forward and backward passes, so we should be able to leverage existing techniques used for training to our goal of distributing inference.\nTo me, it makes intuitive sense that most information about LLM parallelism is related to training. This is a non-interactive operation (not sensitive to latency) that is quite compute-intensive. LLM Inference, on the other hand, is typically used in applications where the time to response is critical (such as chatGPT) - in a distributed setting, unless the problem we are tackling is embarrassingly parallelizable, we can expect the need to synchronize data between nodes, which will add latency to our application. However, I believe there are still several applications that would benefit from distributing inference at the cost of latency (such as batch operations that require LLM capabilities), and I’m also keen to explore how fast I can perform inference using multiple nodes.\nIn this blog series, I will explore existing algorithms to perform distributed inference for LLMs, their limitations and tradeoffs, and (try to) implement them from scratch to understand them better.\nHow many ways can you split a matrix? Naturally, there already exists plenty of work dedicated to distributing the training of LLMs, and as I said, many of the concepts and techniques developed for parallel training should be applicable to inference as well. OpenAI provides a great starting point to understand how researchers in the field have tackled this problem.\nThere are several techniques to perform parallel training of LLMs, but for the sake of simplicity, I will start by dividing the problem into 3 options:\nData Parallelism - where we load the entire model in each node but only pass part of the training data. Gradients are then averaged across workers. This option is not applicable for inference since LLMs are auto-regressive and, as such, require previously generated tokens to predict a given token.\nPipeline Parallelism - here, we load a subset of the model’s layers in each node and sequentially pass activations until we reach the network’s end. Intuitively, because there is a sequential dependency between nodes, we can expect that there will be no gains from concurrently processing data in multiple machines. However, we can split a large model that would typically not fit in memory in multiple machines since the parameter size per node should be 1/n for n nodes.\nTensor Parallelism - in tensor parallelism, we leverage the fact that matrix multiplication is a problem that is trivially parallelizable. Each node maintains all layers of the network, but for each layer, only a fraction of the parameters. In this configuration, we can process each layer in parallel in multiple nodes, reducing the memory usage and the compute requirements in each node.\nThere are other ways to distribute the computation of LLMs, but for now, I will focus first on understanding tensor parallelism, specifically the MegatronLM paper from Nvidia.\nTensor Parallelism with MegatronLM One of the key ideas of the MegatronLM paper is that we can leverage the mathematical properties of matrix multiplication to distribute our computation. The paper focuses on applying their ideas to the transformer model. Specifically, we will be applying these ideas to the GPT-2 architecture, which is a decoder-only transformer.\nTo better understand the paper, I implemented some ideas using Andrej’s GPT-2 nanoGPT implementation as a reference. The code can be found here, and I will reference it as I go through the paper.\n—\nDecoder transformers are composed of several blocks of self-attention and fully connected layers. We will focus on section 3 of the paper, where the authors provide an implementation both for the fully connected layer (MLP) and for the attention layer.\nParallelizing the MLP A multilayer perceptron is a kind of neural network. It is a fully connected network with a non-linear activation function. In GPT-2, this MLP has 2 linear layers with a GeLU non-linearity in between them.\n1 2 3 4 5 def forward(self, x): _, T, _ = x.size() x = self.c_fc(x) # linear layer x = self.gelu(x) # activation x = self.c_proj(x) # linear layer The key idea from the paper is that we can split the parameters of each linear layer in this MLP across nodes, but how we split the parameters and reconcile the results is worth explaining.\nThe following illustration shows a simple representation of a multiplication of an input 1x2 matrix by a 2x2 matrix, the result of which will be a 1x2 matrix.\nIf we split the 2x2 matrix across its rows, each node would have a 1x2 result matrix, but the values would not be correct until we summed the matrices from both nodes. If, on the other hand, we split the 2x2 matrix across its columns, as shown in the illustration above, each node will have a 1x1 matrix with the correct final result.\nIn the transformer MLP, we can think of the blue matrix as the input and the 2x2 matrix (in yellow and red) as the parameters of the linear layer. The number of columns represents the hidden dimension of the linear layer, and the number of rows must match the columns of our input. The GPT-2 paper actually uses 4 * n_embed for the hidden dimension, but for simplicity’s sake, I’ll use n_hidden = n_embed.\nTo calculate the output of the MLP, we need to multiply the two matrices. This is where we can split the 2x2 matrix across the two columns sending each split to a different node, then send the full input matrix to both nodes, and finally, in each node, independently (and in parallel), calculate each result element (green and pink elements in the figure).\nThis approach splits the parameter matrix of the linear layer across multiple nodes, reducing the amount of memory and computation that each node must do. The entire input matrix must still be passed to all nodes, so we get sublinear distribution gains from this operation.\nThe diagram above represents part of Figure 3a from the paper. In it, the authors explain the key idea of first splitting the A parameter matrix by columns and the B matrix by rows. As we saw before, if we split the second element of matrix multiplication by columns, each node will have a result matrix with a subset of the columns, whereas if we split by rows, the result matrix will have the correct result shape, but we will need to reduce (synchronize) the data between all nodes to arrive at the correct result.\nThe authors opted for this order of operations to minimize synchronization points. A GeLU is a non-linear operation that can only be safely applied in parallel if we split the first linear layer across its columns, resulting in the equation:\n$$ [Y1, Y2] = [GeLU(X A1), GeLU(X A2)] $$\nWhere A is the parameters of the linear layer, A1 has the first half of the columns of the matrix, and A2 is the second half. As we saw before, we can independently calculate the resulting Y matrix in different nodes.\nHad the authors split A across its rows, they would need to synchronize data before applying the non-linear activation function - since each node would have a result matrix with the correct format after the first linear layer, but with only half of the parameters having been summed.\nFor the second linear layer, we must partition its parameter matrix along its rows (matrices B1 and B2) so that matrix multiplication rules are preserved.\nAfter applying the non-linearity, each node will have a resulting 1x2 matrix (lime green and brown in node 1, dark red and magenta in node 2) that contains the result in the correct format, but to calculate the final result of the MLP block, we must take both matrices from the 2 nodes and add them (using an all_reduce operation) to get the final result.\nThis is the key strategy that the authors also use in the self-attention block. I won’t go into details of that here, but will briefly mention it as we go through the code.\nAdapting NanoGPT to run in parallel To apply the ideas discussed in the previous section, I adapted the NanoGPT implementation so that the MLP and the self-attention blocks can be split across several nodes. My goal was to create a minimally viable implementation, that favors readibility against completeness or sophistication.\nThe entire GPT-2 implementation is still contained in a single file, distributed_model.py. The relevant files from the repo are:\n1 2 3 4 5 6 . ├── run.py \u003c- CLI application wrapper ├── distributed_inference.py \u003c- Launches the processes and waits for the results. ├── distributed_model.py \u003c- GPT-2 with tensor parallelism ├── distributed_state.py \u003c- Simulated distributed all reduce └── download_model.py \u003c- Use this to download the GPT-2 124M model from HF As an example, for this first PoC, I’m not using Torch’s all_reduce operation, but rather simulating it using Python’s BaseManager process. The idea here is that we launch several processes, and they all publish their intermediate results to the base manager, and only when all have submited their results, they all get the final, reduced version of the matrix:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 def all_reduce(self, op_id: str, tensor: torch.Tensor): with self.cv: if op_id not in self.state: self.state[op_id] = (tensor, 1) else: state_tensor, count = self.state[op_id] self.state[op_id] = ( torch.add(state_tensor, tensor), count + 1) if self.state[op_id][1] \u003c self.number_of_workers: self.cv.wait() else: self.cv.notify_all() return self.state[op_id][0] For each operation (uniquely identified using the op_id - using the worker index, the block name, and the current token index) we sum the input matrix with the existing state, and block the caller on a condition variable. When the final worker calls the all_reduce function, we notify all blocked users and return the final matrix.\nThe points where we use all reduce are the synchronization points between machines. The amount of data we need to transfer and the inter-node latency will be the bottlenecks for the forward pass performance.\nThis first version of the code does not address all aspects presented in the paper. I simply focused on implementing the distributed MLP and self-attention blocks. Other aspects such as word, position and transposed embeddings were not distributed yet.\nThere are 2 key areas worth exploring in this code. 1) how I’m loading the model shard in each node; and 2) how I’m setting the MLP and Self-Attention parameter sizes. Let’s look at model loading first.\nSharded model loading Model loading is done in the load_layers function of the distributed_mode.py module. I leverage safetensors to load a slice of each layer selectively. As an example:\n1 2 3 4 if \"mlp.c_fc.weight\" in layer: # Partition by column. This will convert the slice to a Tensor object tensor_slice = f.get_slice(layer) tensors[layer] = tensor_slice[:, mlp_start_idx:mlp_end_idx] Here I’m loading a slice of the first linear layer of the MLP block by loading only the columns allocated to this worker node. Start and end indeces for the columns are calculated using this helper function:\n1 2 3 4 5 6 7 8 def get_worker_partition(C: int = 768, worker_index: int = 0, number_of_workers: int = 1): partition_size = C // number_of_workers # Calculate the \"chunk\" that this node will process partition_start = worker_index * partition_size partition_end = partition_start + partition_size return (partition_start, partition_end) ","wordCount":"2067","inLanguage":"en","datePublished":"2023-10-21T16:31:22+01:00","dateModified":"2023-10-21T16:31:22+01:00","author":{"@type":"Person","name":"José Côrte-Real"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://mlashcorp.github.io/no_generals_problem/posts/1/"},"publisher":{"@type":"Organization","name":"NGP","logo":{"@type":"ImageObject","url":"https://mlashcorp.github.io/no_generals_problem/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://mlashcorp.github.io/no_generals_problem accesskey=h title="NGP (Alt + H)">NGP</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://mlashcorp.github.io/no_generals_problem/archives title=Archive><span>Archive</span></a></li><li><a href=https://mlashcorp.github.io/no_generals_problem/search/ title=Search><span>Search</span></a></li><li><a href=https://mlashcorp.github.io/no_generals_problem/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://mlashcorp.github.io/no_generals_problem>Home</a>&nbsp;»&nbsp;<a href=https://mlashcorp.github.io/no_generals_problem/posts/>Posts</a></div><h1 class=post-title>LLM Distributed Inference for Fun and not Profit - Part 1</h1><div class=post-meta><span title='2023-10-21 16:31:22 +0100 +0100'>October 21, 2023</span>&nbsp;·&nbsp;10 min&nbsp;·&nbsp;José Côrte-Real</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#how-many-ways-can-you-split-a-matrix aria-label="How many ways can you split a matrix?">How many ways can you split a matrix?</a></li><li><a href=#tensor-parallelism-with-megatronlm aria-label="Tensor Parallelism with MegatronLM">Tensor Parallelism with MegatronLM</a><ul><li><a href=#parallelizing-the-mlp aria-label="Parallelizing the MLP">Parallelizing the MLP</a></li></ul></li><li><a href=#adapting-nanogpt-to-run-in-parallel aria-label="Adapting NanoGPT to run in parallel">Adapting NanoGPT to run in parallel</a><ul><li><a href=#sharded-model-loading aria-label="Sharded model loading">Sharded model loading</a></li></ul></li></ul></div></details></div><div class=post-content><p><img loading=lazy src=ngp_1.jpeg alt="distributed inference"></p><p>Recently, I got very interested in learning and understanding how LLMs work. Specifically, I was curious about how LLMs are used at scale (i.e., with multiple nodes).</p><p>One of the best resources I found to learn about LLMs was Andrej Karpathy&rsquo;s YouTube video, where he <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY">created GPT-2 from scratch</a>
.</p><blockquote><p>Also, if, like me, your fundamentals on neural networks are a bit rusty, he has a <a href=https://karpathy.ai/zero-to-hero.html>whole series</a> where he builds up from the basic concepts of neural nets up to the transformer model.</p></blockquote><p>—</p><p>Having learned the basics of how a Transformer model works, it was time to dive into the gory details of distributing the computation of an LLM.</p><p>My intuition told me to start with the problem of distributing the forward pass (inference, in which we provide input to an LLM and get generated text as a result), if nothing else, because it felt like an easier problem to tackle compared to distributed training.</p><p>Most of the work I&rsquo;ve seen focuses on Training parallelism - using multiple machines to accelerate the process of training the transformer model. However, training a neural network requires both forward and backward passes, so we should be able to leverage existing techniques used for training to our goal of distributing inference.</p><p>To me, it makes intuitive sense that most information about LLM parallelism is related to training. This is a non-interactive operation (not sensitive to latency) that is quite compute-intensive. LLM Inference, on the other hand, is typically used in applications where the time to response is critical (such as chatGPT) - in a distributed setting, unless the problem we are tackling is <a href=https://en.wikipedia.org/wiki/Embarrassingly_parallel>embarrassingly parallelizable</a>, we can expect the need to synchronize data between nodes, which will add latency to our application. However, I believe there are still several applications that would benefit from distributing inference at the cost of latency (such as batch operations that require LLM capabilities), and I&rsquo;m also keen to explore how fast I can perform inference using multiple nodes.</p><p>In this blog series, I will explore existing algorithms to perform distributed inference for LLMs, their limitations and tradeoffs, and (try to) implement them from scratch to understand them better.</p><h2 id=how-many-ways-can-you-split-a-matrix>How many ways can you split a matrix?<a hidden class=anchor aria-hidden=true href=#how-many-ways-can-you-split-a-matrix>#</a></h2><p>Naturally, there already exists plenty of work dedicated to distributing the training of LLMs, and as I said, many of the concepts and techniques developed for parallel training should be applicable to inference as well. OpenAI provides a <a href=https://openai.com/research/techniques-for-training-large-neural-networks>great starting point</a> to understand how researchers in the field have tackled this problem.</p><p>There are several techniques to perform parallel training of LLMs, but for the sake of simplicity, I will start by dividing the problem into 3 options:</p><ol><li><p>Data Parallelism - where we load the entire model in each node but only pass part of the training data. Gradients are then averaged across workers. This option is not applicable for inference since LLMs are <a href="https://www.investopedia.com/terms/a/autoregressive.asp#:~:text=A%20statistical%20model%20is%20autoregressive,based%20on%20its%20past%20performance.">auto-regressive</a> and, as such, require previously generated tokens to predict a given token.</p></li><li><p>Pipeline Parallelism - here, we load a subset of the model&rsquo;s layers in each node and sequentially pass activations until we reach the network&rsquo;s end. Intuitively, because there is a sequential dependency between nodes, we can expect that there will be no gains from concurrently processing data in multiple machines. However, we can split a large model that would typically not fit in memory in multiple machines since the parameter size per node should be 1/n for n nodes.</p></li><li><p>Tensor Parallelism - in tensor parallelism, we leverage the fact that matrix multiplication is a problem that is trivially parallelizable. Each node maintains all layers of the network, but for each layer, only a fraction of the parameters. In this configuration, we can process each layer in parallel in multiple nodes, reducing the memory usage and the compute requirements in each node.</p></li></ol><p>There are other ways to distribute the computation of LLMs, but for now, I will focus first on understanding tensor parallelism, specifically the <a href=https://arxiv.org/abs/1909.08053>MegatronLM paper from Nvidia</a>.</p><h2 id=tensor-parallelism-with-megatronlm>Tensor Parallelism with MegatronLM<a hidden class=anchor aria-hidden=true href=#tensor-parallelism-with-megatronlm>#</a></h2><p>One of the key ideas of the MegatronLM paper is that we can leverage the mathematical properties of matrix multiplication to distribute our computation. The paper focuses on applying their ideas to the transformer model. Specifically, we will be applying these ideas to the GPT-2 architecture, which is a decoder-only transformer.</p><p>To better understand the paper, I implemented some ideas using Andrej&rsquo;s GPT-2 nanoGPT implementation as a reference. The code can be found <a href=https://github.com/mlashcorp/distributed-inference>here</a>, and I will reference it as I go through the paper.</p><p>—</p><p>Decoder transformers are composed of several blocks of self-attention and fully connected layers. We will focus on section 3 of the paper, where the authors provide an implementation both for the fully connected layer (MLP) and for the attention layer.</p><p><img loading=lazy src=decoder.png#center alt=decoder></p><h3 id=parallelizing-the-mlp>Parallelizing the MLP<a hidden class=anchor aria-hidden=true href=#parallelizing-the-mlp>#</a></h3><p>A multilayer perceptron is a kind of neural network. It is a fully connected network with a non-linear activation function. In GPT-2, this MLP has 2 linear layers with a GeLU non-linearity in between them.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>def forward(self, x):
</span></span><span class=line><span class=cl>    _, T, _ = x.size() 
</span></span><span class=line><span class=cl>    x = self.c_fc(x)   # linear layer
</span></span><span class=line><span class=cl>    x = self.gelu(x)   # activation
</span></span><span class=line><span class=cl>    x = self.c_proj(x) # linear layer
</span></span></code></pre></td></tr></table></div></div><p>The key idea from the paper is that we can split the parameters of each linear layer in this MLP across nodes, but how we split the parameters and reconcile the results is worth explaining.</p><p>The following illustration shows a simple representation of a multiplication of an input 1x2 matrix by a 2x2 matrix, the result of which will be a 1x2 matrix.</p><p><img loading=lazy src=gemm-1.png#center alt=gemm></p><p>If we split the 2x2 matrix across its rows, each node would have a 1x2 result matrix, but the values would not be correct until we summed the matrices from both nodes. If, on the other hand, we split the 2x2 matrix across its columns, as shown in the illustration above, each node will have a 1x1 matrix with the correct final result.</p><p>In the transformer MLP, we can think of the blue matrix as the input and the 2x2 matrix (in yellow and red) as the parameters of the linear layer. The number of columns represents the hidden dimension of the linear layer, and the number of rows must match the columns of our input. The GPT-2 paper actually uses 4 * n_embed for the hidden dimension, but for simplicity&rsquo;s sake, I&rsquo;ll use n_hidden = n_embed.</p><p>To calculate the output of the MLP, we need to <a href=https://en.wikipedia.org/wiki/Matrix_multiplication>multiply the two matrices</a>. This is where we can split the 2x2 matrix across the two columns sending each split to a different node, then send the full input matrix to both nodes, and finally, in each node, independently (and in parallel), calculate each result element (green and pink elements in the figure).</p><blockquote><p>This approach splits the parameter matrix of the linear layer across multiple nodes, reducing the amount of memory and computation that each node must do. The entire input matrix must still be passed to all nodes, so we get sublinear distribution gains from this operation.</p></blockquote><p><img loading=lazy src=fig3a.png#center alt=fig3a></p><p>The diagram above represents part of Figure 3a from the paper. In it, the authors explain the key idea of first splitting the A parameter matrix by columns and the B matrix by rows. As we saw before, if we split the second element of matrix multiplication by columns, each node will have a result matrix with a subset of the columns, whereas if we split by rows, the result matrix will have the correct result shape, but we will need to reduce (synchronize) the data between all nodes to arrive at the correct result.</p><p>The authors opted for this order of operations to minimize synchronization points. A GeLU is a non-linear operation that can only be safely applied in parallel if we split the first linear layer across its columns, resulting in the equation:</p><p>$$ [Y1, Y2] = [GeLU(X A1), GeLU(X A2)] $$</p><p>Where A is the parameters of the linear layer, A1 has the first half of the columns of the matrix, and A2 is the second half. As we saw before, we can independently calculate the resulting Y matrix in different nodes.</p><p>Had the authors split A across its rows, they would need to synchronize data before applying the non-linear activation function - since each node would have a result matrix with the correct format after the first linear layer, but with only half of the parameters having been summed.</p><p>For the second linear layer, we must partition its parameter matrix along its rows (matrices B1 and B2) so that matrix multiplication rules are preserved.</p><p><img loading=lazy src=gemm-2.png#center alt=gemm></p><p>After applying the non-linearity, each node will have a resulting 1x2 matrix (lime green and brown in node 1, dark red and magenta in node 2) that contains the result in the correct format, but to calculate the final result of the MLP block, we must take both matrices from the 2 nodes and add them (using an <a href=https://pytorch.org/docs/stable/distributed.html#torch.distributed.all_reduce>all_reduce operation</a>) to get the final result.</p><p>This is the key strategy that the authors also use in the self-attention block. I won&rsquo;t go into details of that here, but will briefly mention it as we go through the code.</p><h2 id=adapting-nanogpt-to-run-in-parallel>Adapting NanoGPT to run in parallel<a hidden class=anchor aria-hidden=true href=#adapting-nanogpt-to-run-in-parallel>#</a></h2><p>To apply the ideas discussed in the previous section, I adapted the <a href=https://github.com/karpathy/nanoGPT>NanoGPT implementation</a> so that the MLP and the self-attention blocks can be split across several nodes. My goal was to create a minimally viable implementation, that favors readibility against completeness or sophistication.</p><p>The entire GPT-2 implementation is still contained in a single file, distributed_model.py. The relevant files from the repo are:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>.
</span></span><span class=line><span class=cl>├── run.py                     &lt;- CLI application wrapper
</span></span><span class=line><span class=cl>├── distributed_inference.py   &lt;- Launches the processes and waits for the results.
</span></span><span class=line><span class=cl>├── distributed_model.py       &lt;- GPT-2 with tensor parallelism
</span></span><span class=line><span class=cl>├── distributed_state.py       &lt;- Simulated distributed all reduce
</span></span><span class=line><span class=cl>└── download_model.py          &lt;- Use this to download the GPT-2 124M model from HF
</span></span></code></pre></td></tr></table></div></div><p>As an example, for this first PoC, I&rsquo;m not using Torch&rsquo;s all_reduce operation, but rather simulating it using Python&rsquo;s <a href=https://docs.python.org/3/library/multiprocessing.html#multiprocessing.managers.BaseManager>BaseManager process</a>. The idea here is that we launch several processes, and they all publish their intermediate results to the base manager, and only when all have submited their results, they all get the final, reduced version of the matrix:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=k>def</span> <span class=nf>all_reduce</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>op_id</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span> <span class=n>tensor</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=bp>self</span><span class=o>.</span><span class=n>cv</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>op_id</span> <span class=ow>not</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>state</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>state</span><span class=p>[</span><span class=n>op_id</span><span class=p>]</span> <span class=o>=</span> <span class=p>(</span><span class=n>tensor</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>state_tensor</span><span class=p>,</span> <span class=n>count</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>state</span><span class=p>[</span><span class=n>op_id</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>state</span><span class=p>[</span><span class=n>op_id</span><span class=p>]</span> <span class=o>=</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>torch</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>state_tensor</span><span class=p>,</span> <span class=n>tensor</span><span class=p>),</span> <span class=n>count</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>state</span><span class=p>[</span><span class=n>op_id</span><span class=p>][</span><span class=mi>1</span><span class=p>]</span> <span class=o>&lt;</span> <span class=bp>self</span><span class=o>.</span><span class=n>number_of_workers</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>cv</span><span class=o>.</span><span class=n>wait</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>cv</span><span class=o>.</span><span class=n>notify_all</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>state</span><span class=p>[</span><span class=n>op_id</span><span class=p>][</span><span class=mi>0</span><span class=p>]</span>
</span></span></code></pre></td></tr></table></div></div><p>For each operation (uniquely identified using the op_id - using the worker index, the block name, and the current token index) we sum the input matrix with the existing state, and block the caller on a condition variable. When the final worker calls the all_reduce function, we notify all blocked users and return the final matrix.</p><blockquote><p>The points where we use all reduce are the synchronization points between machines. The amount of data we need to transfer and the inter-node latency will be the bottlenecks for the forward pass performance.</p></blockquote><p>This first version of the code does not address all aspects presented in the paper. I simply focused on implementing the distributed MLP and self-attention blocks. Other aspects such as word, position and transposed embeddings were not distributed yet.</p><p>There are 2 key areas worth exploring in this code. 1) how I&rsquo;m loading the model shard in each node; and 2) how I&rsquo;m setting the MLP and Self-Attention parameter sizes. Let&rsquo;s look at model loading first.</p><h3 id=sharded-model-loading>Sharded model loading<a hidden class=anchor aria-hidden=true href=#sharded-model-loading>#</a></h3><p>Model loading is done in the load_layers function of the distributed_mode.py module. I leverage safetensors to load a slice of each layer selectively. As an example:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=k>if</span> <span class=s2>&#34;mlp.c_fc.weight&#34;</span> <span class=ow>in</span> <span class=n>layer</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=c1># Partition by column. This will convert the slice to a Tensor object</span>
</span></span><span class=line><span class=cl>    <span class=n>tensor_slice</span> <span class=o>=</span> <span class=n>f</span><span class=o>.</span><span class=n>get_slice</span><span class=p>(</span><span class=n>layer</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>tensors</span><span class=p>[</span><span class=n>layer</span><span class=p>]</span> <span class=o>=</span> <span class=n>tensor_slice</span><span class=p>[:,</span> <span class=n>mlp_start_idx</span><span class=p>:</span><span class=n>mlp_end_idx</span><span class=p>]</span>
</span></span></code></pre></td></tr></table></div></div><p>Here I&rsquo;m loading a slice of the first linear layer of the MLP block by loading only the columns allocated to this worker node. Start and end indeces for the columns are calculated using this helper function:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl>        <span class=k>def</span> <span class=nf>get_worker_partition</span><span class=p>(</span><span class=n>C</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>768</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                 <span class=n>worker_index</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                 <span class=n>number_of_workers</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>partition_size</span> <span class=o>=</span> <span class=n>C</span> <span class=o>//</span> <span class=n>number_of_workers</span>
</span></span><span class=line><span class=cl>            <span class=c1># Calculate the &#34;chunk&#34; that this node will process</span>
</span></span><span class=line><span class=cl>            <span class=n>partition_start</span> <span class=o>=</span> <span class=n>worker_index</span> <span class=o>*</span> <span class=n>partition_size</span>
</span></span><span class=line><span class=cl>            <span class=n>partition_end</span> <span class=o>=</span> <span class=n>partition_start</span> <span class=o>+</span> <span class=n>partition_size</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=p>(</span><span class=n>partition_start</span><span class=p>,</span> <span class=n>partition_end</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div></div><footer class=post-footer><ul class=post-tags></ul><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share LLM Distributed Inference for Fun and not Profit - Part 1 on twitter" href="https://twitter.com/intent/tweet/?text=LLM%20Distributed%20Inference%20for%20Fun%20and%20not%20Profit%20-%20Part%201&amp;url=https%3a%2f%2fmlashcorp.github.io%2fno_generals_problem%2fposts%2f1%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share LLM Distributed Inference for Fun and not Profit - Part 1 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fmlashcorp.github.io%2fno_generals_problem%2fposts%2f1%2f&amp;title=LLM%20Distributed%20Inference%20for%20Fun%20and%20not%20Profit%20-%20Part%201&amp;summary=LLM%20Distributed%20Inference%20for%20Fun%20and%20not%20Profit%20-%20Part%201&amp;source=https%3a%2f%2fmlashcorp.github.io%2fno_generals_problem%2fposts%2f1%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share LLM Distributed Inference for Fun and not Profit - Part 1 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fmlashcorp.github.io%2fno_generals_problem%2fposts%2f1%2f&title=LLM%20Distributed%20Inference%20for%20Fun%20and%20not%20Profit%20-%20Part%201"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share LLM Distributed Inference for Fun and not Profit - Part 1 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fmlashcorp.github.io%2fno_generals_problem%2fposts%2f1%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share LLM Distributed Inference for Fun and not Profit - Part 1 on whatsapp" href="https://api.whatsapp.com/send?text=LLM%20Distributed%20Inference%20for%20Fun%20and%20not%20Profit%20-%20Part%201%20-%20https%3a%2f%2fmlashcorp.github.io%2fno_generals_problem%2fposts%2f1%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share LLM Distributed Inference for Fun and not Profit - Part 1 on telegram" href="https://telegram.me/share/url?text=LLM%20Distributed%20Inference%20for%20Fun%20and%20not%20Profit%20-%20Part%201&amp;url=https%3a%2f%2fmlashcorp.github.io%2fno_generals_problem%2fposts%2f1%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share LLM Distributed Inference for Fun and not Profit - Part 1 on ycombinator" href="https://news.ycombinator.com/submitlink?t=LLM%20Distributed%20Inference%20for%20Fun%20and%20not%20Profit%20-%20Part%201&u=https%3a%2f%2fmlashcorp.github.io%2fno_generals_problem%2fposts%2f1%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://mlashcorp.github.io/no_generals_problem>NGP</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>